{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of MSSW on SEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEA dataset locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrupt_sea_path = '../Datasets_concept_drift/synthetic_data/abrupt_drift/sea_1_abrupt_drift_0_noise_balanced.arff'\n",
    "gradual_sea_paths = [\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_20.arff'\n",
    "]\n",
    "all_sea_data_paths = [abrupt_sea_path] + gradual_sea_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accept and preprocess SEA datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  4.636430  6.639370  0.066740  b'groupB'\n",
      "99996  4.251993  3.351235  5.652197  b'groupA'\n",
      "99997  4.131405  6.371722  3.125554  b'groupB'\n",
      "99998  1.404214  4.392506  9.298558  b'groupA'\n",
      "99999  7.231749  8.770465  3.925490  b'groupB'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\mssw\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  2.074508  1.775662  1.318589  b'groupA'\n",
      "99996  4.636430  6.639370  0.066740  b'groupB'\n",
      "99997  4.251993  3.351235  5.652197  b'groupA'\n",
      "99998  4.131405  6.371722  3.125554  b'groupB'\n",
      "99999  1.404214  4.392506  9.298558  b'groupA'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\mssw\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  2.261206  1.404770  3.977088  b'groupA'\n",
      "99996  0.795885  8.915077  6.892585  b'groupB'\n",
      "99997  0.353597  1.289198  0.001943  b'groupA'\n",
      "99998  6.540503  5.698432  7.743243  b'groupB'\n",
      "99999  2.074508  1.775662  1.318589  b'groupA'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\mssw\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m sea_testing_batches \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sea_path \u001b[38;5;129;01min\u001b[39;00m all_sea_data_paths:\n\u001b[1;32m---> 10\u001b[0m     df_x, df_y \u001b[38;5;241m=\u001b[39m \u001b[43maccepting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_clean_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msea_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     df_y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(LabelEncoder()\u001b[38;5;241m.\u001b[39mfit_transform(df_y))\n\u001b[0;32m     13\u001b[0m     df_x_ref, df_x_test, df_y_ref, df_y_test \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmodel_selection\u001b[38;5;241m.\u001b[39mtrain_test_split(\n\u001b[0;32m     14\u001b[0m         df_x, df_y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\clustering-drift-detection\\mssw\\accepting.py:35\u001b[0m, in \u001b[0;36mget_clean_df\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_clean_df\u001b[39m(file_path):\n\u001b[1;32m---> 35\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43maccept_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     df_x, df_y \u001b[38;5;241m=\u001b[39m prepare_df_data(df)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_x, df_y\n",
      "File \u001b[1;32m~\\PycharmProjects\\clustering-drift-detection\\mssw\\accepting.py:8\u001b[0m, in \u001b[0;36maccept_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccept_data\u001b[39m(file_path):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Accept an arff file and return its contents in a pandas dataframe\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43marff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m, df)\n",
      "File \u001b[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:802\u001b[0m, in \u001b[0;36mloadarff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    800\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_loadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mofile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ofile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f:  \u001b[38;5;66;03m# only close what we opened\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:867\u001b[0m, in \u001b[0;36m_loadarff\u001b[1;34m(ofile)\u001b[0m\n\u001b[0;32m    863\u001b[0m         row, dialect \u001b[38;5;241m=\u001b[39m split_data_line(raw, dialect)\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([attr[i]\u001b[38;5;241m.\u001b[39mparse_data(row[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m elems])\n\u001b[1;32m--> 867\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mofile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# No error should happen here: it is a bug otherwise\u001b[39;00m\n\u001b[0;32m    869\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(a, [(a\u001b[38;5;241m.\u001b[39mname, a\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr])\n",
      "File \u001b[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:857\u001b[0m, in \u001b[0;36m_loadarff.<locals>.generator\u001b[1;34m(row_iter, delim)\u001b[0m\n\u001b[0;32m    854\u001b[0m elems \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(ni))\n\u001b[0;32m    856\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m raw \u001b[38;5;129;01min\u001b[39;00m row_iter:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# We do not abstract skipping comments and empty lines for\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# performance reasons.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r_comment\u001b[38;5;241m.\u001b[39mmatch(raw) \u001b[38;5;129;01mor\u001b[39;00m r_empty\u001b[38;5;241m.\u001b[39mmatch(raw):\n\u001b[0;32m    861\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\encodings\\cp1252.py:22\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalDecoder\u001b[39;00m(codecs\u001b[38;5;241m.\u001b[39mIncrementalDecoder):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import accepting\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "sea_reference_batches = {}\n",
    "sea_testing_batches = {}\n",
    "for sea_path in all_sea_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(sea_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    reference_data = df_x_ref.to_numpy()\n",
    "    testing_data = df_x_test.to_numpy()\n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    sea_reference_batches[sea_path] = ref_batches\n",
    "    sea_testing_batches[sea_path] = test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best tol and max_iter in SEA (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mssw_preprocessing\n",
    "import kmeans_verbose_helpers\n",
    "\n",
    "\n",
    "weighted_joined_reference_data, _, _ = mssw_preprocessing.mssw_preprocess(\n",
    "    sea_reference_batches[abrupt_sea_path], sea_testing_batches[abrupt_sea_path])\n",
    "\n",
    "filename = 'sea_new_output.txt'\n",
    "print(common_helpers)\n",
    "kmeans_verbose_helpers.write_verbose_kmeans_to_file(filename, weighted_joined_reference_data,\n",
    "                             n_clusters=2, n_init=100, max_iter=500, tol=0, random_state=1053)\n",
    "output_dicts = kmeans_verbose_helpers.convert_kmeans_output_file_to_dicts(filename, n_init=100)\n",
    "kmeans_verbose_helpers.print_stats_from_kmeans_output_dicts(output_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mssw_eval\n",
    "\n",
    "\n",
    "sea_stats = {}\n",
    "for sea_path in all_sea_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        mssw_eval.all_drifting_batches_randomness_robust(\n",
    "        sea_reference_batches[sea_path],\n",
    "        sea_testing_batches[sea_path],\n",
    "        n_clusters=2,\n",
    "        n_init=100,\n",
    "        max_iter=550,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    sea_stats[sea_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('SEA STATS')\n",
    "print(sea_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in sea_stats.items():\n",
    "#     data_filename = data_path.split('/')[-1]\n",
    "#     type_of_data = data_path.split('/')[2].split('_')[0]  # synthetic or real-world\n",
    "#     dataset_name = data_filename.split('_')[0]  # sea, agraw1, agraw2\n",
    "#     drift_type = data_path.split('/')[3].split('_')[0]\n",
    "#     drift_width = '0' if drift_type == 'abrupt' else data_filename.split('_')[-1].split('.')[0]\n",
    "#     drift_width = 0.5 if drift_width == '05' else float(drift_width)\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'exclude'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(550)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
