{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of MSSW on SEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEA dataset locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrupt_sea_path = '../Datasets_concept_drift/synthetic_data/abrupt_drift/sea_1_abrupt_drift_0_noise_balanced.arff'\n",
    "gradual_sea_paths = [\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_20.arff'\n",
    "]\n",
    "all_sea_data_paths = [abrupt_sea_path] + gradual_sea_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accept and preprocess SEA datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  4.636430  6.639370  0.066740  b'groupB'\n",
      "99996  4.251993  3.351235  5.652197  b'groupA'\n",
      "99997  4.131405  6.371722  3.125554  b'groupB'\n",
      "99998  1.404214  4.392506  9.298558  b'groupA'\n",
      "99999  7.231749  8.770465  3.925490  b'groupB'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  2.074508  1.775662  1.318589  b'groupA'\n",
      "99996  4.636430  6.639370  0.066740  b'groupB'\n",
      "99997  4.251993  3.351235  5.652197  b'groupA'\n",
      "99998  4.131405  6.371722  3.125554  b'groupB'\n",
      "99999  1.404214  4.392506  9.298558  b'groupA'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  2.261206  1.404770  3.977088  b'groupA'\n",
      "99996  0.795885  8.915077  6.892585  b'groupB'\n",
      "99997  0.353597  1.289198  0.001943  b'groupA'\n",
      "99998  6.540503  5.698432  7.743243  b'groupB'\n",
      "99999  2.074508  1.775662  1.318589  b'groupA'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  7.680433  9.606008  5.626119  b'groupB'\n",
      "99996  1.336234  3.864737  1.698313  b'groupA'\n",
      "99997  0.541297  9.975611  6.822081  b'groupB'\n",
      "99998  1.709441  4.517255  8.083470  b'groupA'\n",
      "99999  0.778983  9.326919  2.996122  b'groupA'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  8.630346  3.981509  1.040496  b'groupB'\n",
      "99996  0.656783  5.144250  9.648631  b'groupA'\n",
      "99997  8.299312  7.466245  5.592216  b'groupB'\n",
      "99998  2.728767  5.140916  0.727831  b'groupA'\n",
      "99999  8.852912  3.855575  6.128628  b'groupB'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df         attrib1   attrib2   attrib3      class\n",
      "0      7.308782  4.100808  2.077148  b'groupB'\n",
      "1      5.833539  0.422983  7.616747  b'groupA'\n",
      "2      1.397627  6.949480  8.052278  b'groupB'\n",
      "3      2.750299  0.753878  6.105915  b'groupA'\n",
      "4      2.049135  6.233638  1.847071  b'groupB'\n",
      "...         ...       ...       ...        ...\n",
      "99995  8.479838  4.037801  7.474048  b'groupB'\n",
      "99996  0.587347  2.725972  8.395393  b'groupA'\n",
      "99997  1.297264  9.227339  6.843533  b'groupB'\n",
      "99998  2.009023  5.305785  1.423271  b'groupA'\n",
      "99999  9.680480  4.642564  3.628668  b'groupB'\n",
      "\n",
      "[100000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from eval_helpers import accepting\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "sea_reference_batches = {}\n",
    "sea_testing_batches = {}\n",
    "for sea_path in all_sea_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(sea_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    reference_data = df_x_ref.to_numpy()\n",
    "    testing_data = df_x_test.to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    \n",
    "    sea_reference_batches[sea_path] = ref_batches\n",
    "    sea_testing_batches[sea_path] = test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best tol and max_iter in SEA (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n"
     ]
    }
   ],
   "source": [
    "from eval_helpers import kmeans_verbose_helpers\n",
    "\n",
    "\n",
    "def write_kmeans_results_ucdd_helper(output_filename_no_extension, ref_batches, n_init, max_iter, tol, random_state):\n",
    "    # dummy = [np.asarray(1), np.asarray(2), np.asarray(3)]\n",
    "    combinations = []\n",
    "    for i in range(3):\n",
    "    #     combinations.append(np.vstack((dummy[i], dummy[(i + 1) % 3])))\n",
    "        combinations.append(np.vstack((ref_batches[i], ref_batches[(i + 1) % 3])))\n",
    "        \n",
    "    all_results_from_combinations = []\n",
    "    for i, combination in enumerate(combinations):\n",
    "        filename = output_filename_no_extension + str(i) + '.txt'\n",
    "        print('filename', filename)\n",
    "        kmeans_verbose_helpers.write_verbose_kmeans_to_file(result_filename=output_filename_no_extension + str(i) + '.txt',\n",
    "                                     data_to_cluster=combination,\n",
    "                                     n_clusters=2, n_init=n_init, max_iter=max_iter, tol=tol, random_state=random_state)\n",
    "        output_dicts = kmeans_verbose_helpers.convert_kmeans_output_file_to_dicts(filename, n_init=n_init)\n",
    "        all_results_from_combinations.append(output_dicts)\n",
    "        kmeans_verbose_helpers.print_stats_from_kmeans_output_dicts(output_dicts)\n",
    "        \n",
    "    kmeans_verbose_helpers.print_stats_from_all_combinations(all_results_from_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename sea_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 77\n",
      "minimum initial inertia: 3807.187051337405\n",
      "maximum initial inertia: 8731.074732381052\n",
      "number of unique final inertia values: 19\n",
      "minimum final inertia: 3554.5339834841066\n",
      "maximum final inertia: 3652.064139150284\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename sea_output1.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 54\n",
      "minimum initial inertia: 3963.912240518916\n",
      "maximum initial inertia: 8146.817583220964\n",
      "number of unique final inertia values: 5\n",
      "minimum final inertia: 3522.635321428201\n",
      "maximum final inertia: 3632.905333372651\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename sea_output2.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 71\n",
      "minimum initial inertia: 3887.476245307372\n",
      "maximum initial inertia: 8426.6262988874\n",
      "number of unique final inertia values: 14\n",
      "minimum final inertia: 3555.233746395852\n",
      "maximum final inertia: 3613.5161478893065\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "{'total_max_iterations': 77, 'total_min_init_inertia': 3807.187051337405, 'total_max_init_inertia': 8731.074732381052, 'total_min_final_inertia': 3522.635321428201, 'total_max_final_inertia': 3652.064139150284, 'total_num_convergences': 300, 'total_num_strict_convergences': 300, 'total_num_tol_based_convergences': 0}\n"
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('sea_output', sea_reference_batches[abrupt_sea_path], n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use them for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try the parallel code instead of for loops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sea_path ../Datasets_concept_drift/synthetic_data/abrupt_drift/sea_1_abrupt_drift_0_noise_balanced.arff\n",
      "random_state\n",
      "0\n",
      "sea_path ../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_05.arff\n",
      "random_state\n",
      "0\n",
      "sea_path ../Datasets_concept_drift/synthetic_data/gradual_drift/sea_1_gradual_drift_0_noise_balanced_1.arff\n",
      "random_state\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[53], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sea_path \u001B[38;5;129;01min\u001B[39;00m all_sea_data_paths:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msea_path\u001B[39m\u001B[38;5;124m'\u001B[39m, sea_path)\n\u001B[1;32m---> 26\u001B[0m     drifts_detected \u001B[38;5;241m=\u001B[39m \u001B[43mucdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43msea_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mabrupt_sea_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43msea_testing_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mabrupt_sea_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_ref_batches_drift\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m77000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     all_drift_detections_parallel\u001B[38;5;241m.\u001B[39mappend(drifts_detected)\n\u001B[0;32m     38\u001B[0m end_time_parallel \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:190\u001B[0m, in \u001B[0;36mall_drifting_batches\u001B[1;34m(reference_data_batches, testing_data_batches, min_ref_batches_drift, additional_check, n_init, max_iter, tol, random_state, parallel)\u001B[0m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;124;03mFind all drift locations based on the given reference and testing batches\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;124;03m    an entry is True if drift was detected there and False otherwise\u001B[39;00m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parallel:\n\u001B[1;32m--> 190\u001B[0m     drifts_detected \u001B[38;5;241m=\u001B[39m \u001B[43mall_drifting_batches_parallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreference_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtesting_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_ref_batches_drift\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_state\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:256\u001B[0m, in \u001B[0;36mall_drifting_batches_parallel\u001B[1;34m(reference_data_batches, testing_data_batches, min_ref_batches_drift, additional_check, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m    254\u001B[0m drifts_1d \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Pool() \u001B[38;5;28;01mas\u001B[39;00m pool:\n\u001B[1;32m--> 256\u001B[0m     drifts_1d \u001B[38;5;241m=\u001B[39m \u001B[43mpool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstarmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconcept_drift_detected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpool_iterables\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    258\u001B[0m drifts_1d_arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(drifts_1d)\n\u001B[0;32m    259\u001B[0m \u001B[38;5;66;03m# each column of the 2d array represents results for one testing batch\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py:372\u001B[0m, in \u001B[0;36mPool.starmap\u001B[1;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstarmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    367\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001B[39;00m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001B[39;00m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001B[39;00m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m--> 372\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstarmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    766\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n\u001B[0;32m    767\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py:762\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 762\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:574\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    572\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 574\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from core import ucdd\n",
    "import importlib\n",
    "importlib.reload(ucdd)\n",
    "import time\n",
    "\n",
    "# all_drift_detections = []\n",
    "# start_time_regular = time.time()\n",
    "# for sea_path in all_sea_data_paths:\n",
    "#     drifts_detected = ucdd.all_drifting_batches(\n",
    "#         sea_reference_batches[abrupt_sea_path],\n",
    "#         sea_testing_batches[abrupt_sea_path],\n",
    "#         min_ref_batches_drift=0.3,\n",
    "#         additional_check=True,\n",
    "#         n_init=100,\n",
    "#         max_iter=77000,\n",
    "#         tol=0,\n",
    "#         random_state=0\n",
    "#     )\n",
    "#     all_drift_detections.append(drifts_detected)\n",
    "# end_time_regular = time.time()\n",
    "\n",
    "all_drift_detections_parallel = []\n",
    "start_time_parallel = time.time()\n",
    "for sea_path in all_sea_data_paths:\n",
    "    print('sea_path', sea_path)\n",
    "    drifts_detected = ucdd.all_drifting_batches(\n",
    "        sea_reference_batches[abrupt_sea_path],\n",
    "        sea_testing_batches[abrupt_sea_path],\n",
    "        min_ref_batches_drift=0.3,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=77000,\n",
    "        tol=0,\n",
    "        random_state=0,\n",
    "        parallel=True\n",
    "    )\n",
    "    all_drift_detections_parallel.append(drifts_detected)\n",
    "end_time_parallel = time.time()\n",
    "\n",
    "# print('REGULAR RUN')\n",
    "# print('time(sec):', end_time_regular - start_time_regular)\n",
    "# print('result:')\n",
    "# print(all_drift_detections)\n",
    "\n",
    "print('PARALLEL RUN')\n",
    "print('time(sec):', end_time_parallel - start_time_parallel)\n",
    "print('result:')\n",
    "print(all_drift_detections_parallel)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state\n",
      "0\n",
      "#### TEST BATCH 0 of 7 ####\n",
      "#### TEST BATCH 1 of 7 ####\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m sea_stats \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sea_path \u001B[38;5;129;01min\u001B[39;00m all_sea_data_paths:\n\u001B[0;32m      6\u001B[0m     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m----> 7\u001B[0m         \u001B[43mucdd_eval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches_randomness_robust\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43msea_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43msea_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43msea_testing_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43msea_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_ref_batches_drift\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m77000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_drift_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_runs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m     sea_stats[sea_path] \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mruns_results_bool\u001B[39m\u001B[38;5;124m'\u001B[39m: runs_results_bool,\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinal_fpr_mean\u001B[39m\u001B[38;5;124m'\u001B[39m: final_fpr_mean,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatency_std_err\u001B[39m\u001B[38;5;124m'\u001B[39m: latency_std_err\n\u001B[0;32m     24\u001B[0m     }\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSEA STATS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd_eval.py:57\u001B[0m, in \u001B[0;36mall_drifting_batches_randomness_robust\u001B[1;34m(reference_data_batches, testing_data_batches, min_ref_batches_drift, additional_check, n_init, max_iter, tol, true_drift_idx, first_random_state, min_runs, std_err_threshold)\u001B[0m\n\u001B[0;32m     55\u001B[0m random_state \u001B[38;5;241m=\u001B[39m first_random_state\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m num_runs \u001B[38;5;241m<\u001B[39m min_runs \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(fpr_std_err, latency_std_err) \u001B[38;5;241m>\u001B[39m std_err_threshold:\n\u001B[1;32m---> 57\u001B[0m     drifting_batches_bool \u001B[38;5;241m=\u001B[39m \u001B[43mucdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreference_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtesting_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_ref_batches_drift\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_ref_batches_drift\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# print('drifting_batches_bool')\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# print(drifting_batches_bool)\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     drift_locations \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(drifting_batches_bool))[drifting_batches_bool]\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:194\u001B[0m, in \u001B[0;36mall_drifting_batches\u001B[1;34m(reference_data_batches, testing_data_batches, min_ref_batches_drift, additional_check, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m    192\u001B[0m num_ref_drifts \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# how many training batches signal drift against this testing batch\u001B[39;00m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, ref_window \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(reference_data_batches):\n\u001B[1;32m--> 194\u001B[0m     drift_here \u001B[38;5;241m=\u001B[39m \u001B[43mconcept_drift_detected\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drift_here:\n\u001B[0;32m    197\u001B[0m         num_ref_drifts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:143\u001B[0m, in \u001B[0;36mconcept_drift_detected\u001B[1;34m(ref_window, test_window, additional_check, n_init, max_iter, tol, random_state, threshold, debug)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcept_drift_detected\u001B[39m(\n\u001B[0;32m    118\u001B[0m         ref_window,\n\u001B[0;32m    119\u001B[0m         test_window,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    126\u001B[0m         debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    127\u001B[0m ):\n\u001B[0;32m    128\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m    Detect whether a concept drift occurred based on one reference and one testing window\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;124;03m    :return: true if drift is detected based on the two windows, false otherwise\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    142\u001B[0m     ref_plus, ref_minus, test_plus, test_minus \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m--> 143\u001B[0m         \u001B[43mjoin_predict_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBETA MINUS (ref+, ref-, test-)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    147\u001B[0m     beta_minus, beta_minus_additional \u001B[38;5;241m=\u001B[39m compute_beta(\n\u001B[0;32m    148\u001B[0m         ref_plus, ref_minus, test_minus)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:63\u001B[0m, in \u001B[0;36mjoin_predict_split\u001B[1;34m(ref_window, test_window, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     58\u001B[0m window_union \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack((ref_window, test_window))\n\u001B[0;32m     60\u001B[0m \u001B[38;5;66;03m# print('n_init', n_init, 'max_iter', max_iter, 'tol', tol)\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# predict their label values\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m \u001B[43mKMeans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow_union\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# split values by predicted label and window\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m split_back_to_windows(window_union, predicted_labels, ref_window\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], test_window\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:996\u001B[0m, in \u001B[0;36m_BaseKMeans.fit_predict\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    974\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001B[39;00m\n\u001B[0;32m    975\u001B[0m \n\u001B[0;32m    976\u001B[0m \u001B[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;124;03m        Index of the cluster each sample belongs to.\u001B[39;00m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 996\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1414\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitialization complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1416\u001B[0m \u001B[38;5;66;03m# run a k-means once\u001B[39;00m\n\u001B[1;32m-> 1417\u001B[0m labels, inertia, centers, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43mkmeans_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1418\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1419\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1420\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcenters_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1421\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1422\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1423\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1424\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1425\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_n_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1426\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1428\u001B[0m \u001B[38;5;66;03m# determine if these results are the best so far\u001B[39;00m\n\u001B[0;32m   1429\u001B[0m \u001B[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001B[39;00m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001B[39;00m\n\u001B[0;32m   1431\u001B[0m \u001B[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001B[39;00m\n\u001B[0;32m   1432\u001B[0m \u001B[38;5;66;03m# permuted labels, due to rounding errors)\u001B[39;00m\n\u001B[0;32m   1433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_inertia \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   1434\u001B[0m     inertia \u001B[38;5;241m<\u001B[39m best_inertia\n\u001B[0;32m   1435\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_same_clustering(labels, best_labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters)\n\u001B[0;32m   1436\u001B[0m ):\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:683\u001B[0m, in \u001B[0;36m_kmeans_single_lloyd\u001B[1;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001B[0m\n\u001B[0;32m    677\u001B[0m                 \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    678\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverged at iteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: center shift \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    679\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcenter_shift_tot\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m within tolerance \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    680\u001B[0m                 )\n\u001B[0;32m    681\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 683\u001B[0m     \u001B[43mlabels_old\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m labels\n\u001B[0;32m    685\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m strict_convergence:\n\u001B[0;32m    686\u001B[0m     \u001B[38;5;66;03m# rerun E-step so that predicted labels match cluster centers\u001B[39;00m\n\u001B[0;32m    687\u001B[0m     lloyd_iter(\n\u001B[0;32m    688\u001B[0m         X,\n\u001B[0;32m    689\u001B[0m         sample_weight,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    697\u001B[0m         update_centers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    698\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "\n",
    "\n",
    "sea_stats = {}\n",
    "for sea_path in all_sea_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        sea_reference_batches[sea_path],\n",
    "        sea_testing_batches[sea_path],\n",
    "        min_ref_batches_drift=0.3,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=77000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    sea_stats[sea_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('SEA STATS')\n",
    "print(sea_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'min_ref_batches_drift': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in sea_stats.items():\n",
    "#     data_filename = data_path.split('/')[-1]\n",
    "#     type_of_data = data_path.split('/')[2].split('_')[0]  # synthetic or real-world\n",
    "#     dataset_name = data_filename.split('_')[0]  # sea, agraw1, agraw2\n",
    "#     drift_type = data_path.split('/')[3].split('_')[0]\n",
    "#     drift_width = '0' if drift_type == 'abrupt' else data_filename.split('_')[-1].split('.')[0]\n",
    "#     drift_width = 0.5 if drift_width == '05' else float(drift_width)\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'exclude'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['min_ref_batches_drift'].append(0.3)\n",
    "    final_result_dict['additional_check'].append('yes')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(550)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('sea_jupyter_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}