{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of UCDD on AGRAW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 dataset locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrupt_agraw1_path = '../Datasets_concept_drift/synthetic_data/abrupt_drift/agraw1_1_abrupt_drift_0_noise_balanced.arff'\n",
    "gradual_agraw1_paths = [\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_20.arff'\n",
    "]\n",
    "\n",
    "all_agraw1_data_paths = [abrupt_agraw1_path] + gradual_agraw1_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accept and preprocess AGRAW1 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "from eval_helpers import accepting\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "agraw1_exclude_reference_batches = {}\n",
    "agraw1_exclude_testing_batches = {}\n",
    "agraw1_onehot_reference_batches = {}\n",
    "agraw1_onehot_testing_batches = {}\n",
    "agraw1_target_reference_batches = {}\n",
    "agraw1_target_testing_batches = {}\n",
    "\n",
    "agraw1_reference_label_batches = {}\n",
    "agraw1_testing_label_batches = {}\n",
    "\n",
    "# agraw1 with categories excluded\n",
    "for file_path in all_agraw1_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(file_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "\n",
    "    reference_labels = df_y_ref.to_numpy()\n",
    "    testing_labels = df_y_test.to_numpy()\n",
    "    \n",
    "    df_x_ref_num, df_x_ref_cat = accepting.divide_numeric_categorical(df_x_ref)\n",
    "    df_x_test_num, df_x_test_cat = accepting.divide_numeric_categorical(df_x_test)\n",
    "    \n",
    "    reference_data = df_x_ref_num.to_numpy()\n",
    "    testing_data = df_x_test_num.to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    ref_label_batches = np.array_split(reference_labels, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    test_label_batches = np.array_split(testing_labels, num_test_batches)\n",
    "    \n",
    "    agraw1_exclude_reference_batches[file_path] = ref_batches\n",
    "    agraw1_reference_label_batches[file_path] = ref_label_batches\n",
    "    agraw1_exclude_testing_batches[file_path] = test_batches\n",
    "    agraw1_testing_label_batches[file_path] = test_label_batches\n",
    "    \n",
    "# print('agraw1 exclude')\n",
    "# print(agraw1_exclude_reference_batches)\n",
    "# print(agraw1_exclude_testing_batches)\n",
    "\n",
    "# agraw1 with categories onehot encoded\n",
    "for file_path in all_agraw1_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(file_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    df_x_ref_num, df_x_ref_cat = accepting.divide_numeric_categorical(df_x_ref)\n",
    "    df_x_test_num, df_x_test_cat = accepting.divide_numeric_categorical(df_x_test)\n",
    "    \n",
    "    ref_index = df_x_ref_cat.index\n",
    "    test_index = df_x_test_cat.index\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(df_x_ref_cat)\n",
    "    df_x_ref_cat_transformed = pd.DataFrame(encoder.transform(df_x_ref_cat))\n",
    "    df_x_test_cat_transformed = pd.DataFrame(encoder.transform(df_x_test_cat))\n",
    "    df_x_ref_cat_transformed.set_index(ref_index, inplace=True)\n",
    "    df_x_test_cat_transformed.set_index(test_index, inplace=True)\n",
    "    \n",
    "    reference_data = df_x_ref_num.join(df_x_ref_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    testing_data = df_x_test_num.join(df_x_test_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    agraw1_onehot_reference_batches[file_path] = ref_batches\n",
    "    agraw1_onehot_testing_batches[file_path] = test_batches\n",
    "    \n",
    "# print('agraw1 onehot')\n",
    "# print(agraw1_onehot_reference_batches)\n",
    "# print(agraw1_onehot_testing_batches)\n",
    "\n",
    "# agraw1 with categories target encoded\n",
    "for file_path in all_agraw1_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(file_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    df_x_ref_num, df_x_ref_cat = accepting.divide_numeric_categorical(df_x_ref)\n",
    "    df_x_test_num, df_x_test_cat = accepting.divide_numeric_categorical(df_x_test)\n",
    "    \n",
    "    ref_index = df_x_ref_cat.index\n",
    "    test_index = df_x_test_cat.index\n",
    "    encoder = TargetEncoder()\n",
    "    encoder.fit(df_x_ref_cat, df_y_ref)\n",
    "    df_x_ref_cat_transformed = pd.DataFrame(encoder.transform(df_x_ref_cat))\n",
    "    df_x_test_cat_transformed = pd.DataFrame(encoder.transform(df_x_test_cat))\n",
    "    df_x_ref_cat_transformed.set_index(ref_index, inplace=True)\n",
    "    df_x_test_cat_transformed.set_index(test_index, inplace=True)\n",
    "    \n",
    "    reference_data = df_x_ref_num.join(df_x_ref_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    testing_data = df_x_test_num.join(df_x_test_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    agraw1_target_reference_batches[file_path] = ref_batches\n",
    "    agraw1_target_testing_batches[file_path] = test_batches\n",
    "    \n",
    "# print('agraw1 target')\n",
    "# print(agraw1_target_reference_batches)\n",
    "# print(agraw1_target_testing_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something\n"
     ]
    }
   ],
   "source": [
    "from eval_helpers import kmeans_verbose_helpers\n",
    "\n",
    "\n",
    "def write_kmeans_results_ucdd_helper(output_filename_no_extension, ref_batches, n_init, max_iter, tol, random_state):\n",
    "    # dummy = [np.asarray(1), np.asarray(2), np.asarray(3)]\n",
    "    combinations = []\n",
    "    for i in range(3):\n",
    "    #     combinations.append(np.vstack((dummy[i], dummy[(i + 1) % 3])))\n",
    "        combinations.append(np.vstack((ref_batches[i], ref_batches[(i + 1) % 3])))\n",
    "        \n",
    "    all_results_from_combinations = []\n",
    "    for i, combination in enumerate(combinations):\n",
    "        filename = output_filename_no_extension + str(i) + '.txt'\n",
    "        print('filename', filename)\n",
    "        kmeans_verbose_helpers.write_verbose_kmeans_to_file(result_filename=output_filename_no_extension + str(i) + '.txt',\n",
    "                                     data_to_cluster=combination,\n",
    "                                     n_clusters=2, n_init=n_init, max_iter=max_iter, tol=tol, random_state=random_state)\n",
    "        output_dicts = kmeans_verbose_helpers.convert_kmeans_output_file_to_dicts(filename, n_init=n_init)\n",
    "        all_results_from_combinations.append(output_dicts)\n",
    "        kmeans_verbose_helpers.print_stats_from_kmeans_output_dicts(output_dicts)\n",
    "        \n",
    "    kmeans_verbose_helpers.print_stats_from_all_combinations(all_results_from_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 with categories excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best tol and max_iter (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename agraw1_exclude_new_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 18\n",
      "minimum initial inertia: 8347.093515830069\n",
      "maximum initial inertia: 18690.074352335017\n",
      "number of unique final inertia values: 3\n",
      "minimum final inertia: 6832.419247273033\n",
      "maximum final inertia: 6832.419270386381\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_exclude_new_output1.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 18\n",
      "minimum initial inertia: 8747.52244428163\n",
      "maximum initial inertia: 17134.178458342543\n",
      "number of unique final inertia values: 3\n",
      "minimum final inertia: 6820.074930844902\n",
      "maximum final inertia: 6820.0750120817775\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_exclude_new_output2.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 15\n",
      "minimum initial inertia: 8327.374918776613\n",
      "maximum initial inertia: 17457.421016637192\n",
      "number of unique final inertia values: 2\n",
      "minimum final inertia: 6806.4197402168365\n",
      "maximum final inertia: 6806.419740216837\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "{'total_max_iterations': 18, 'total_min_init_inertia': 8327.374918776613, 'total_max_init_inertia': 18690.074352335017, 'total_min_final_inertia': 6806.4197402168365, 'total_max_final_inertia': 6832.419270386381, 'total_num_convergences': 300, 'total_num_strict_convergences': 300, 'total_num_tol_based_convergences': 0}\n"
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('agraw1_exclude_new_output', agraw1_exclude_reference_batches[abrupt_agraw1_path],\n",
    "                                 n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 1.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 1.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 1.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 1.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 1.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 1.0\n",
      "AGRAW1 STATS\n",
      "{'../Datasets_concept_drift/synthetic_data/abrupt_drift/agraw1_1_abrupt_drift_0_noise_balanced.arff': {'runs_results_bool': [[False, False, False, False, False, False, False], [False, False, False, False, False, False, False]], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 1.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_05.arff': {'runs_results_bool': [[False, False, False, False, False, False, False], [False, False, False, False, False, False, False]], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 1.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_1.arff': {'runs_results_bool': [[False, False, False, False, False, False, False], [False, False, False, False, False, False, False]], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 1.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_5.arff': {'runs_results_bool': [[False, False, False, False, False, False, False], [False, False, False, False, False, False, False]], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 1.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_10.arff': {'runs_results_bool': [[False, False, False, False, False, False, False], [False, False, False, False, False, False, False]], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 1.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_20.arff': {'runs_results_bool': [[False, False, False, False, False, False, False], [False, False, False, False, False, False, False]], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 1.0, 'latency_std_err': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "\n",
    "\n",
    "agraw1_exclude_stats = {}\n",
    "agraw1_path = all_agraw1_data_paths[-1]\n",
    "# for agraw1_path in all_agraw1_data_paths:\n",
    "#     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "#         ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "#         agraw1_reference_batches[agraw1_path],\n",
    "#         agraw1_testing_batches[agraw1_path],\n",
    "#         min_ref_batches_drift=0.3,\n",
    "#         additional_check=True,\n",
    "#         n_init=100,\n",
    "#         max_iter=18000,\n",
    "#         tol=0,\n",
    "#         true_drift_idx=2,\n",
    "#         min_runs=2\n",
    "#     )\n",
    "#     agraw1_stats[agraw1_path] = {\n",
    "#         'runs_results_bool': runs_results_bool,\n",
    "#         'final_fpr_mean': final_fpr_mean,\n",
    "#         'fpr_std_err': fpr_std_err,\n",
    "#         'final_latency_mean': final_latency_mean,\n",
    "#         'latency_std_err': latency_std_err\n",
    "#     }\n",
    "\n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, _, _, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_exclude_reference_batches[agraw1_path],\n",
    "        agraw1_exclude_testing_batches[agraw1_path],\n",
    "        min_ref_batches_drift=0.3,\n",
    "        additional_check=False,\n",
    "        n_init=100,\n",
    "        max_iter=18000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    agraw1_exclude_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 STATS')\n",
    "print(agraw1_exclude_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the obtained results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'min_ref_batches_drift': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in agraw1_exclude_stats.items():\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'exclude'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['min_ref_batches_drift'].append(0.3)\n",
    "    final_result_dict['additional_check'].append('no')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(18000)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('agraw1_exclude_jupyter_results_no_check.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use UCDD directly to obtain inspectable results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_state\n",
      "0\n",
      "pool opened\n"
     ]
    }
   ],
   "source": [
    "agraw1_exclude_stats = {}\n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, all_2d_drifts, all_2d_cluster_classif_accs,\\\n",
    "    final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_exclude_reference_batches[agraw1_path],\n",
    "        agraw1_exclude_testing_batches[agraw1_path],\n",
    "        min_ref_batches_drift=0.3,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=18000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2,\n",
    "        reference_label_batches=agraw1_reference_label_batches[agraw1_path],\n",
    "        testing_label_batches=agraw1_testing_label_batches[agraw1_path]\n",
    "    )\n",
    "    agraw1_exclude_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'all_2d_drifts': all_2d_drifts,\n",
    "        'all_2d_cluster_classif_accs': all_2d_cluster_classif_accs,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 EXCLUDE STATS')\n",
    "print(agraw1_exclude_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5264142857142857\n"
     ]
    }
   ],
   "source": [
    "# np.save('agraw1_exclude_stats_all_widths.npy', agraw1_exclude_stats)\n",
    "\n",
    "excl_stats_dict = np.load('agraw1_exclude_stats_all_widths.npy', allow_pickle=True).item()\n",
    "print(np.mean(excl_stats_dict[all_agraw1_data_paths[0]]['all_2d_cluster_classif_accs'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 with categories onehot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best tol and max_iter (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename agraw1_onehot_new_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 34\n",
      "minimum initial inertia: 104954.65523014727\n",
      "maximum initial inertia: 116645.01047187511\n",
      "number of unique final inertia values: 19\n",
      "minimum final inertia: 58341.71044879218\n",
      "maximum final inertia: 59761.482370558224\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_onehot_new_output1.txt\n",
      "random state: 1053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of results: 100\n",
      "maximum number of iterations: 33\n",
      "minimum initial inertia: 105187.473271114\n",
      "maximum initial inertia: 117235.7046827969\n",
      "number of unique final inertia values: 13\n",
      "minimum final inertia: 58307.80854907961\n",
      "maximum final inertia: 59757.571876885886\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_onehot_new_output2.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 26\n",
      "minimum initial inertia: 104976.092943036\n",
      "maximum initial inertia: 118204.3234742695\n",
      "number of unique final inertia values: 23\n",
      "minimum final inertia: 58330.68910892084\n",
      "maximum final inertia: 59758.880442981055\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "{'total_max_iterations': 34, 'total_min_init_inertia': 104954.65523014727, 'total_max_init_inertia': 118204.3234742695, 'total_min_final_inertia': 58307.80854907961, 'total_max_final_inertia': 59761.482370558224, 'total_num_convergences': 300, 'total_num_strict_convergences': 300, 'total_num_tol_based_convergences': 0}\n"
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('agraw1_onehot_new_output', agraw1_onehot_reference_batches[abrupt_agraw1_path],\n",
    "                                 n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "train_batch_strategy TrainBatchStrategies.MAJORITY\n",
      "random_state\n",
      "0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainBatchStrategies.MAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "TrainBatchStrategies.MAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_init 100 max_iter 34000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 10\u001B[0m\n\u001B[0;32m      6\u001B[0m agraw1_onehot_stats \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agraw1_path \u001B[38;5;129;01min\u001B[39;00m all_agraw1_data_paths:\n\u001B[0;32m      9\u001B[0m     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m---> 10\u001B[0m         \u001B[43mucdd_eval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches_randomness_robust\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43magraw1_onehot_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43magraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43magraw1_onehot_testing_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43magraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTrainBatchStrategies\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMAJORITY\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m34000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_drift_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_runs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     agraw1_onehot_stats[agraw1_path] \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     22\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mruns_results_bool\u001B[39m\u001B[38;5;124m'\u001B[39m: runs_results_bool,\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinal_fpr_mean\u001B[39m\u001B[38;5;124m'\u001B[39m: final_fpr_mean,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatency_std_err\u001B[39m\u001B[38;5;124m'\u001B[39m: latency_std_err\n\u001B[0;32m     27\u001B[0m     }\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAGRAW1 STATS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd_eval.py:57\u001B[0m, in \u001B[0;36mall_drifting_batches_randomness_robust\u001B[1;34m(reference_data_batches, testing_data_batches, train_batch_strategy, additional_check, n_init, max_iter, tol, true_drift_idx, first_random_state, min_runs, std_err_threshold)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m num_runs \u001B[38;5;241m<\u001B[39m min_runs \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(fpr_std_err, latency_std_err) \u001B[38;5;241m>\u001B[39m std_err_threshold:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_batch_strategy\u001B[39m\u001B[38;5;124m'\u001B[39m, train_batch_strategy)\n\u001B[1;32m---> 57\u001B[0m     drifting_batches_bool \u001B[38;5;241m=\u001B[39m \u001B[43mucdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreference_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtesting_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# print('drifting_batches_bool')\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# print(drifting_batches_bool)\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     drift_locations \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(drifting_batches_bool))[drifting_batches_bool]\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:189\u001B[0m, in \u001B[0;36mall_drifting_batches\u001B[1;34m(reference_data_batches, testing_data_batches, train_batch_strategy, additional_check, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m    187\u001B[0m num_ref_drifts \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# how many training batches signal drift against this testing batch\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, ref_window \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(reference_data_batches):\n\u001B[1;32m--> 189\u001B[0m     drift_here \u001B[38;5;241m=\u001B[39m \u001B[43mconcept_drift_detected\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drift_here:\n\u001B[0;32m    192\u001B[0m         num_ref_drifts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:138\u001B[0m, in \u001B[0;36mconcept_drift_detected\u001B[1;34m(ref_window, test_window, additional_check, n_init, max_iter, tol, random_state, threshold, debug)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcept_drift_detected\u001B[39m(\n\u001B[0;32m    113\u001B[0m         ref_window,\n\u001B[0;32m    114\u001B[0m         test_window,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    121\u001B[0m         debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    122\u001B[0m ):\n\u001B[0;32m    123\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03m    Detect whether a concept drift occurred based on one reference and one testing window\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m    :return: true if drift is detected based on the two windows, false otherwise\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    137\u001B[0m     ref_plus, ref_minus, test_plus, test_minus \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m--> 138\u001B[0m         \u001B[43mjoin_predict_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBETA MINUS (ref+, ref-, test-)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    142\u001B[0m     beta_minus, beta_minus_additional \u001B[38;5;241m=\u001B[39m compute_beta(\n\u001B[0;32m    143\u001B[0m         ref_plus, ref_minus, test_minus)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:63\u001B[0m, in \u001B[0;36mjoin_predict_split\u001B[1;34m(ref_window, test_window, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_init\u001B[39m\u001B[38;5;124m'\u001B[39m, n_init, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_iter\u001B[39m\u001B[38;5;124m'\u001B[39m, max_iter, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtol\u001B[39m\u001B[38;5;124m'\u001B[39m, tol)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# predict their label values\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m \u001B[43mKMeans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow_union\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# split values by predicted label and window\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m split_back_to_windows(window_union, predicted_labels, ref_window\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], test_window\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:996\u001B[0m, in \u001B[0;36m_BaseKMeans.fit_predict\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    974\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001B[39;00m\n\u001B[0;32m    975\u001B[0m \n\u001B[0;32m    976\u001B[0m \u001B[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;124;03m        Index of the cluster each sample belongs to.\u001B[39;00m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 996\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1410\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1406\u001B[0m best_inertia, best_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1408\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_init):\n\u001B[0;32m   1409\u001B[0m     \u001B[38;5;66;03m# Initialize centers\u001B[39;00m\n\u001B[1;32m-> 1410\u001B[0m     centers_init \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_centroids\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m   1412\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1413\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[0;32m   1414\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitialization complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:952\u001B[0m, in \u001B[0;36m_BaseKMeans._init_centroids\u001B[1;34m(self, X, x_squared_norms, init, random_state, init_size, n_centroids)\u001B[0m\n\u001B[0;32m    949\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(init, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m init \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mk-means++\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 952\u001B[0m     centers, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_kmeans_plusplus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    954\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(init, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m init \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrandom\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    959\u001B[0m     seeds \u001B[38;5;241m=\u001B[39m random_state\u001B[38;5;241m.\u001B[39mpermutation(n_samples)[:n_clusters]\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:224\u001B[0m, in \u001B[0;36m_kmeans_plusplus\u001B[1;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001B[0m\n\u001B[0;32m    221\u001B[0m np\u001B[38;5;241m.\u001B[39mclip(candidate_ids, \u001B[38;5;28;01mNone\u001B[39;00m, closest_dist_sq\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, out\u001B[38;5;241m=\u001B[39mcandidate_ids)\n\u001B[0;32m    223\u001B[0m \u001B[38;5;66;03m# Compute distances to center candidates\u001B[39;00m\n\u001B[1;32m--> 224\u001B[0m distance_to_candidates \u001B[38;5;241m=\u001B[39m \u001B[43m_euclidean_distances\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcandidate_ids\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_norm_squared\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msquared\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m    226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# update closest distances squared and potential for each candidate\u001B[39;00m\n\u001B[0;32m    229\u001B[0m np\u001B[38;5;241m.\u001B[39mminimum(closest_dist_sq, distance_to_candidates, out\u001B[38;5;241m=\u001B[39mdistance_to_candidates)\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:369\u001B[0m, in \u001B[0;36m_euclidean_distances\u001B[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001B[0m\n\u001B[0;32m    366\u001B[0m     distances \u001B[38;5;241m=\u001B[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    368\u001B[0m     \u001B[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001B[39;00m\n\u001B[1;32m--> 369\u001B[0m     distances \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43msafe_sparse_dot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdense_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    370\u001B[0m     distances \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m XX\n\u001B[0;32m    371\u001B[0m     distances \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m YY\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:152\u001B[0m, in \u001B[0;36msafe_sparse_dot\u001B[1;34m(a, b, dense_output)\u001B[0m\n\u001B[0;32m    150\u001B[0m         ret \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(a, b)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 152\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    155\u001B[0m     sparse\u001B[38;5;241m.\u001B[39missparse(a)\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(b)\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m dense_output\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(ret, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoarray\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    159\u001B[0m ):\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39mtoarray()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "\n",
    "\n",
    "print('...')\n",
    "agraw1_onehot_stats = {}\n",
    "\n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_onehot_reference_batches[agraw1_path],\n",
    "        agraw1_onehot_testing_batches[agraw1_path],\n",
    "        min_ref_batches_drift=0.6,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=34000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    agraw1_onehot_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 STATS')\n",
    "print(agraw1_onehot_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the obtained results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'min_ref_batches_drift': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in agraw1_onehot_stats.items():\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'onehot'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['min_ref_batches_drift'].append(0.6)\n",
    "    final_result_dict['additional_check'].append('yes')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(34000)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('agraw1_onehot_jupyter_results_majority.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use UCDD directly to obtain inspectable results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.25\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.25\n",
      "AGRAW1 ONEHOT STATS\n",
      "{'../Datasets_concept_drift/synthetic_data/abrupt_drift/agraw1_1_abrupt_drift_0_noise_balanced.arff': {'runs_results_bool': [[False, False, True, False, False, False, False], [False, False, True, False, False, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50265, 0.5044 , 0.50465, 0.5042 , 0.50405, 0.50515, 0.50405],\n",
      "       [0.50165, 0.503  , 0.5035 , 0.5009 , 0.50435, 0.503  , 0.5005 ],\n",
      "       [0.50095, 0.5008 , 0.5001 , 0.50095, 0.5009 , 0.50155, 0.50515]]), array([[0.50265, 0.5044 , 0.50465, 0.5042 , 0.50405, 0.50515, 0.50405],\n",
      "       [0.50165, 0.503  , 0.5035 , 0.5009 , 0.50435, 0.503  , 0.5005 ],\n",
      "       [0.50095, 0.5008 , 0.5001 , 0.50095, 0.5009 , 0.50155, 0.50515]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_05.arff': {'runs_results_bool': [[False, False, True, False, False, False, False], [False, False, True, False, False, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50265, 0.5044 , 0.5046 , 0.5042 , 0.50405, 0.5051 , 0.50405],\n",
      "       [0.50165, 0.503  , 0.50345, 0.5009 , 0.5044 , 0.50295, 0.50045],\n",
      "       [0.50095, 0.5008 , 0.50015, 0.50095, 0.5009 , 0.5015 , 0.5051 ]]), array([[0.50265, 0.5044 , 0.5046 , 0.5042 , 0.50405, 0.5051 , 0.50405],\n",
      "       [0.50165, 0.503  , 0.50345, 0.5009 , 0.5044 , 0.50295, 0.50045],\n",
      "       [0.50095, 0.5008 , 0.50015, 0.50095, 0.5009 , 0.5015 , 0.5051 ]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_1.arff': {'runs_results_bool': [[False, False, True, False, False, False, False], [False, False, True, False, False, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50265, 0.5044 , 0.50455, 0.50425, 0.50395, 0.5051 , 0.50415],\n",
      "       [0.50165, 0.503  , 0.50345, 0.501  , 0.50455, 0.50295, 0.5005 ],\n",
      "       [0.50095, 0.5008 , 0.50015, 0.501  , 0.5009 , 0.5015 , 0.50515]]), array([[0.50265, 0.5044 , 0.50455, 0.50425, 0.50395, 0.5051 , 0.50415],\n",
      "       [0.50165, 0.503  , 0.50345, 0.501  , 0.50455, 0.50295, 0.5005 ],\n",
      "       [0.50095, 0.5008 , 0.50015, 0.501  , 0.5009 , 0.5015 , 0.50515]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_5.arff': {'runs_results_bool': [[False, False, True, False, False, False, False], [False, False, True, False, False, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50265, 0.5043 , 0.5049 , 0.50405, 0.504  , 0.5051 , 0.5041 ],\n",
      "       [0.50165, 0.5029 , 0.5038 , 0.50125, 0.5045 , 0.503  , 0.5005 ],\n",
      "       [0.50095, 0.5007 , 0.50035, 0.5008 , 0.5008 , 0.5015 , 0.50515]]), array([[0.50265, 0.5043 , 0.5049 , 0.50405, 0.504  , 0.5051 , 0.5041 ],\n",
      "       [0.50165, 0.5029 , 0.5038 , 0.50125, 0.5045 , 0.503  , 0.5005 ],\n",
      "       [0.50095, 0.5007 , 0.50035, 0.5008 , 0.5008 , 0.5015 , 0.50515]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_10.arff': {'runs_results_bool': [[False, False, False, True, True, False, False], [False, False, False, True, True, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False, False,  True,  True, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False, False,  True,  True, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.5027 , 0.5042 , 0.5053 , 0.5054 , 0.50545, 0.50495, 0.50385],\n",
      "       [0.5016 , 0.5028 , 0.5041 , 0.5029 , 0.50385, 0.50275, 0.50205],\n",
      "       [0.5009 , 0.5006 , 0.5021 , 0.50215, 0.5027 , 0.5002 , 0.50025]]), array([[0.5027 , 0.5042 , 0.5053 , 0.5054 , 0.50545, 0.50495, 0.50385],\n",
      "       [0.5016 , 0.5028 , 0.5041 , 0.5029 , 0.50385, 0.50275, 0.50205],\n",
      "       [0.5009 , 0.5006 , 0.5021 , 0.50215, 0.5027 , 0.5002 , 0.50025]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.25, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_20.arff': {'runs_results_bool': [[False, False, False, True, False, False, False], [False, False, False, True, False, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False, False,  True, False, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False, False,  True, False, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50255, 0.5047 , 0.50435, 0.5036 , 0.5042 , 0.5051 , 0.505  ],\n",
      "       [0.50175, 0.50255, 0.50485, 0.5028 , 0.50085, 0.50405, 0.5028 ],\n",
      "       [0.5013 , 0.50115, 0.50155, 0.50585, 0.50565, 0.5009 , 0.50145]]), array([[0.50255, 0.5047 , 0.50435, 0.5036 , 0.5042 , 0.5051 , 0.505  ],\n",
      "       [0.50175, 0.50255, 0.50485, 0.5028 , 0.50085, 0.50405, 0.5028 ],\n",
      "       [0.5013 , 0.50115, 0.50155, 0.50585, 0.50565, 0.5009 , 0.50145]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.25, 'latency_std_err': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "agraw1_onehot_stats = {}\n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, all_2d_drifts, all_2d_cluster_classif_accs,\\\n",
    "    final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_onehot_reference_batches[agraw1_path],\n",
    "        agraw1_onehot_testing_batches[agraw1_path],\n",
    "        min_ref_batches_drift=0.3,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=34000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2,\n",
    "        reference_label_batches=agraw1_reference_label_batches[agraw1_path],\n",
    "        testing_label_batches=agraw1_testing_label_batches[agraw1_path]\n",
    "    )\n",
    "    agraw1_onehot_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'all_2d_drifts': all_2d_drifts,\n",
    "        'all_2d_cluster_classif_accs': all_2d_cluster_classif_accs,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 ONEHOT STATS')\n",
    "print(agraw1_onehot_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5031904761904763\n"
     ]
    }
   ],
   "source": [
    "# np.save('agraw1_onehot_stats_all_widths.npy', agraw1_onehot_stats)\n",
    "\n",
    "onehot_stats_dict = np.load('agraw1_onehot_stats_all_widths.npy', allow_pickle=True).item()\n",
    "print(np.mean(onehot_stats_dict[all_agraw1_data_paths[0]]['all_2d_cluster_classif_accs'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 with categories target encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best tol and max_iter (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename agraw1_target_new_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 18\n",
      "minimum initial inertia: 17607.98488650251\n",
      "maximum initial inertia: 29056.647428908935\n",
      "number of unique final inertia values: 21\n",
      "minimum final inertia: 13299.613663873319\n",
      "maximum final inertia: 14276.6351586479\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_target_new_output1.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 22\n",
      "minimum initial inertia: 17728.884037917425\n",
      "maximum initial inertia: 30906.112923249697\n",
      "number of unique final inertia values: 13\n",
      "minimum final inertia: 13290.390633663217\n",
      "maximum final inertia: 14285.170980526265\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_target_new_output2.txt\n",
      "random state: 1053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mwrite_kmeans_results_ucdd_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43magraw1_target_new_output\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magraw1_target_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mabrupt_agraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1053\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 15\u001B[0m, in \u001B[0;36mwrite_kmeans_results_ucdd_helper\u001B[1;34m(output_filename_no_extension, ref_batches, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     13\u001B[0m filename \u001B[38;5;241m=\u001B[39m output_filename_no_extension \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m, filename)\n\u001B[1;32m---> 15\u001B[0m \u001B[43mkmeans_verbose_helpers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_verbose_kmeans_to_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_filename_no_extension\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mdata_to_cluster\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombination\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m output_dicts \u001B[38;5;241m=\u001B[39m kmeans_verbose_helpers\u001B[38;5;241m.\u001B[39mconvert_kmeans_output_file_to_dicts(filename, n_init\u001B[38;5;241m=\u001B[39mn_init)\n\u001B[0;32m     19\u001B[0m all_results_from_combinations\u001B[38;5;241m.\u001B[39mappend(output_dicts)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\kmeans_verbose_helpers.py:32\u001B[0m, in \u001B[0;36mwrite_verbose_kmeans_to_file\u001B[1;34m(result_filename, data_to_cluster, n_clusters, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     29\u001B[0m orig_stdout \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mstdout\n\u001B[0;32m     30\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(result_filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 32\u001B[0m fitted_kmeans \u001B[38;5;241m=\u001B[39m \u001B[43mKMeans\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_to_cluster\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;241m=\u001B[39m orig_stdout\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1414\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitialization complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1416\u001B[0m \u001B[38;5;66;03m# run a k-means once\u001B[39;00m\n\u001B[1;32m-> 1417\u001B[0m labels, inertia, centers, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43mkmeans_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1418\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1419\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1420\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcenters_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1421\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1422\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1423\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1424\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1425\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_n_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1426\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1428\u001B[0m \u001B[38;5;66;03m# determine if these results are the best so far\u001B[39;00m\n\u001B[0;32m   1429\u001B[0m \u001B[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001B[39;00m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001B[39;00m\n\u001B[0;32m   1431\u001B[0m \u001B[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001B[39;00m\n\u001B[0;32m   1432\u001B[0m \u001B[38;5;66;03m# permuted labels, due to rounding errors)\u001B[39;00m\n\u001B[0;32m   1433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_inertia \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   1434\u001B[0m     inertia \u001B[38;5;241m<\u001B[39m best_inertia\n\u001B[0;32m   1435\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_same_clustering(labels, best_labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters)\n\u001B[0;32m   1436\u001B[0m ):\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:661\u001B[0m, in \u001B[0;36m_kmeans_single_lloyd\u001B[1;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001B[0m\n\u001B[0;32m    648\u001B[0m lloyd_iter(\n\u001B[0;32m    649\u001B[0m     X,\n\u001B[0;32m    650\u001B[0m     sample_weight,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    657\u001B[0m     n_threads,\n\u001B[0;32m    658\u001B[0m )\n\u001B[0;32m    660\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[1;32m--> 661\u001B[0m     inertia \u001B[38;5;241m=\u001B[39m \u001B[43m_inertia\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcenters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    662\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, inertia \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minertia\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    664\u001B[0m centers, centers_new \u001B[38;5;241m=\u001B[39m centers_new, centers\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('agraw1_target_new_output', agraw1_target_reference_batches[abrupt_agraw1_path],\n",
    "                                 n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "\n",
    "\n",
    "agraw1_target_stats = {}\n",
    "    \n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_target_reference_batches[agraw1_path],\n",
    "        agraw1_target_testing_batches[agraw1_path],\n",
    "        min_ref_batches_drift=0.6,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=45000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    agraw1_target_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 STATS')\n",
    "print(agraw1_target_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the obtained results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'min_ref_batches_drift': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in agraw1_target_stats.items():\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'target'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['min_ref_batches_drift'].append(0.6)\n",
    "    final_result_dict['additional_check'].append('yes')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(45000)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('agraw1_target_jupyter_results_majority.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use UCDD directly to obtain inspectable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.0\n",
      "final latency mean 0.0\n",
      "entered ucdd eval\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "0\n",
      "pool opened\n",
      "entered ucdd eval loop\n",
      "random_state\n",
      "100\n",
      "pool opened\n",
      "fpr s.e. 0.0\n",
      "latency s.e. 0.0\n",
      "final fpr mean 0.5\n",
      "final latency mean 0.0\n",
      "AGRAW1 TARGET STATS\n",
      "{'../Datasets_concept_drift/synthetic_data/abrupt_drift/agraw1_1_abrupt_drift_0_noise_balanced.arff': {'runs_results_bool': [[False, False, True, False, True, True, True], [False, False, True, False, True, True, True]], 'all_2d_drifts': [array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.5069 , 0.5045 , 0.5203 , 0.5376 , 0.53425, 0.5403 , 0.5356 ],\n",
      "       [0.50355, 0.501  , 0.524  , 0.5408 , 0.53765, 0.54325, 0.53945],\n",
      "       [0.50505, 0.50275, 0.52255, 0.53955, 0.5362 , 0.54185, 0.5375 ]]), array([[0.5069 , 0.5045 , 0.5203 , 0.5376 , 0.53425, 0.5403 , 0.5356 ],\n",
      "       [0.50355, 0.501  , 0.524  , 0.54075, 0.53765, 0.54325, 0.53945],\n",
      "       [0.50505, 0.50275, 0.52255, 0.53955, 0.5362 , 0.54185, 0.5375 ]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_05.arff': {'runs_results_bool': [[False, False, True, False, True, True, True], [False, False, True, False, True, True, True]], 'all_2d_drifts': [array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.5069 , 0.5045 , 0.5203 , 0.5376 , 0.5343 , 0.54025, 0.5356 ],\n",
      "       [0.50355, 0.501  , 0.52395, 0.54075, 0.5377 , 0.54315, 0.53945],\n",
      "       [0.50505, 0.50275, 0.5225 , 0.53955, 0.53625, 0.5418 , 0.5374 ]]), array([[0.5069 , 0.5045 , 0.5203 , 0.5376 , 0.5343 , 0.54025, 0.5356 ],\n",
      "       [0.50355, 0.501  , 0.52395, 0.54075, 0.5377 , 0.54315, 0.53945],\n",
      "       [0.50505, 0.50275, 0.5225 , 0.53955, 0.53625, 0.5418 , 0.5374 ]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_1.arff': {'runs_results_bool': [[False, False, True, False, True, True, True], [False, False, True, False, True, True, True]], 'all_2d_drifts': [array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.5069 , 0.5045 , 0.5204 , 0.53755, 0.53425, 0.5403 , 0.53555],\n",
      "       [0.50355, 0.501  , 0.524  , 0.5407 , 0.53765, 0.54325, 0.5394 ],\n",
      "       [0.50505, 0.50275, 0.52255, 0.5395 , 0.5362 , 0.54195, 0.53755]]), array([[0.5069 , 0.5045 , 0.5204 , 0.53755, 0.53425, 0.5403 , 0.53555],\n",
      "       [0.50355, 0.501  , 0.524  , 0.5407 , 0.53765, 0.54325, 0.5394 ],\n",
      "       [0.50505, 0.50275, 0.52255, 0.5395 , 0.5362 , 0.54195, 0.53755]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_5.arff': {'runs_results_bool': [[False, False, True, False, True, True, True], [False, False, True, False, True, True, True]], 'all_2d_drifts': [array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False,  True, False,  True],\n",
      "       [False, False,  True, False, False,  True, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.5069 , 0.50435, 0.52015, 0.53745, 0.53425, 0.54035, 0.5355 ],\n",
      "       [0.50355, 0.50085, 0.52385, 0.54055, 0.53765, 0.5433 , 0.5394 ],\n",
      "       [0.50505, 0.5026 , 0.52235, 0.5394 , 0.5362 , 0.542  , 0.53765]]), array([[0.5069 , 0.50435, 0.52015, 0.53745, 0.53425, 0.54035, 0.5355 ],\n",
      "       [0.50355, 0.50085, 0.52385, 0.54055, 0.53765, 0.5433 , 0.5394 ],\n",
      "       [0.50505, 0.5026 , 0.52235, 0.5394 , 0.5362 , 0.542  , 0.53765]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_10.arff': {'runs_results_bool': [[False, False, True, False, False, False, False], [False, False, True, False, False, False, False]], 'all_2d_drifts': [array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]]), array([[False, False, False, False, False, False, False],\n",
      "       [False, False,  True, False, False, False, False],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50685, 0.5043 , 0.50405, 0.53235, 0.5367 , 0.5347 , 0.53895],\n",
      "       [0.50355, 0.5007 , 0.5078 , 0.5356 , 0.5394 , 0.53775, 0.54225],\n",
      "       [0.50505, 0.5026 , 0.5058 , 0.53425, 0.53835, 0.53655, 0.54075]]), array([[0.50685, 0.5043 , 0.50405, 0.53235, 0.5367 , 0.5347 , 0.53895],\n",
      "       [0.50355, 0.5007 , 0.5078 , 0.5356 , 0.5394 , 0.5377 , 0.54225],\n",
      "       [0.50505, 0.5026 , 0.5058 , 0.53425, 0.53835, 0.53655, 0.54075]])], 'final_fpr_mean': 0.0, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}, '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_20.arff': {'runs_results_bool': [[False, True, True, False, False, True, True], [False, True, True, False, False, True, True]], 'all_2d_drifts': [array([[False,  True, False, False, False,  True, False],\n",
      "       [False, False,  True, False, False, False,  True],\n",
      "       [False, False, False, False, False, False, False]]), array([[False,  True, False, False, False,  True, False],\n",
      "       [False, False,  True, False, False, False,  True],\n",
      "       [False, False, False, False, False, False, False]])], 'all_2d_cluster_classif_accs': [array([[0.50665, 0.5036 , 0.5018 , 0.51725, 0.5308 , 0.53375, 0.5402 ],\n",
      "       [0.5034 , 0.5003 , 0.50525, 0.5208 , 0.53325, 0.53705, 0.5431 ],\n",
      "       [0.50475, 0.502  , 0.5038 , 0.51945, 0.5323 , 0.5357 , 0.54185]]), array([[0.50665, 0.5036 , 0.5018 , 0.51725, 0.5308 , 0.53375, 0.5402 ],\n",
      "       [0.5034 , 0.5003 , 0.50525, 0.5208 , 0.53325, 0.53705, 0.5431 ],\n",
      "       [0.50475, 0.502  , 0.5038 , 0.51945, 0.5323 , 0.5357 , 0.54185]])], 'final_fpr_mean': 0.5, 'fpr_std_err': 0.0, 'final_latency_mean': 0.0, 'latency_std_err': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "agraw1_target_stats = {}\n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, all_2d_drifts, all_2d_cluster_classif_accs,\\\n",
    "    final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_target_reference_batches[agraw1_path],\n",
    "        agraw1_target_testing_batches[agraw1_path],\n",
    "        min_ref_batches_drift=0.3,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=45000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2,\n",
    "        reference_label_batches=agraw1_reference_label_batches[agraw1_path],\n",
    "        testing_label_batches=agraw1_testing_label_batches[agraw1_path]\n",
    "    )\n",
    "    agraw1_target_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'all_2d_drifts': all_2d_drifts,\n",
    "        'all_2d_cluster_classif_accs': all_2d_cluster_classif_accs,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 TARGET STATS')\n",
    "print(agraw1_target_stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5264095238095239\n",
      "0.5263952380952381\n",
      "0.5264071428571429\n",
      "0.52635\n",
      "0.5232523809523808\n",
      "0.5198595238095238\n"
     ]
    }
   ],
   "source": [
    "# np.save('agraw1_target_stats_all_widths.npy', agraw1_target_stats)\n",
    "\n",
    "target_stats_dict = np.load('agraw1_target_stats_all_widths.npy', allow_pickle=True).item()\n",
    "for i in range(6):\n",
    "    print(np.mean(target_stats_dict[all_agraw1_data_paths[i]]['all_2d_cluster_classif_accs'][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}