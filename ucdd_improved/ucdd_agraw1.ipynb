{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of UCDD on AGRAW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sys' has no attribute '_stdout_'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;241m=\u001B[39m \u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stdout_\u001B[49m\n\u001B[0;32m      7\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39m__stdout__\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(sys\u001B[38;5;241m.\u001B[39mstdout)\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'sys' has no attribute '_stdout_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "\n",
    "sys.stdout = sys._stdout_\n",
    "sys.stdout = sys.__stdout__\n",
    "print(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 dataset locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrupt_agraw1_path = '../Datasets_concept_drift/synthetic_data/abrupt_drift/agraw1_1_abrupt_drift_0_noise_balanced.arff'\n",
    "gradual_agraw1_paths = [\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_05.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_1.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_5.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_10.arff',\n",
    "    '../Datasets_concept_drift/synthetic_data/gradual_drift/agraw1_1_gradual_drift_0_noise_balanced_20.arff'\n",
    "]\n",
    "\n",
    "all_agraw1_data_paths = [abrupt_agraw1_path] + gradual_agraw1_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accept and preprocess AGRAW1 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n",
      "C:\\Users\\jpohl\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\accepting.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column] = df[column].str.decode('utf-8')\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "c:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\category_encoders\\target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "from eval_helpers import accepting\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "agraw1_exclude_reference_batches = {}\n",
    "agraw1_exclude_testing_batches = {}\n",
    "agraw1_onehot_reference_batches = {}\n",
    "agraw1_onehot_testing_batches = {}\n",
    "agraw1_target_reference_batches = {}\n",
    "agraw1_target_testing_batches = {}\n",
    "\n",
    "\n",
    "# agraw1 with categories excluded\n",
    "for file_path in all_agraw1_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(file_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    df_x_ref_num, df_x_ref_cat = accepting.divide_numeric_categorical(df_x_ref)\n",
    "    df_x_test_num, df_x_test_cat = accepting.divide_numeric_categorical(df_x_test)\n",
    "    \n",
    "    reference_data = df_x_ref_num.to_numpy()\n",
    "    testing_data = df_x_test_num.to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    agraw1_exclude_reference_batches[file_path] = ref_batches\n",
    "    agraw1_exclude_testing_batches[file_path] = test_batches\n",
    "    \n",
    "print('agraw1 exclude')\n",
    "print(agraw1_exclude_reference_batches)\n",
    "print(agraw1_exclude_testing_batches)\n",
    "\n",
    "# agraw1 with categories onehot encoded\n",
    "for file_path in all_agraw1_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(file_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    df_x_ref_num, df_x_ref_cat = accepting.divide_numeric_categorical(df_x_ref)\n",
    "    df_x_test_num, df_x_test_cat = accepting.divide_numeric_categorical(df_x_test)\n",
    "    \n",
    "    ref_index = df_x_ref_cat.index\n",
    "    test_index = df_x_test_cat.index\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoder.fit(df_x_ref_cat)\n",
    "    df_x_ref_cat_transformed = pd.DataFrame(encoder.transform(df_x_ref_cat))\n",
    "    df_x_test_cat_transformed = pd.DataFrame(encoder.transform(df_x_test_cat))\n",
    "    df_x_ref_cat_transformed.set_index(ref_index, inplace=True)\n",
    "    df_x_test_cat_transformed.set_index(test_index, inplace=True)\n",
    "    \n",
    "    reference_data = df_x_ref_num.join(df_x_ref_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    testing_data = df_x_test_num.join(df_x_test_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    agraw1_onehot_reference_batches[file_path] = ref_batches\n",
    "    agraw1_onehot_testing_batches[file_path] = test_batches\n",
    "    \n",
    "print('agraw1 onehot')\n",
    "print(agraw1_onehot_reference_batches)\n",
    "print(agraw1_onehot_testing_batches)\n",
    "\n",
    "# agraw1 with categories target encoded\n",
    "for file_path in all_agraw1_data_paths:\n",
    "    df_x, df_y = accepting.get_clean_df(file_path)\n",
    "    df_y = pd.DataFrame(LabelEncoder().fit_transform(df_y))\n",
    "\n",
    "    df_x_ref, df_x_test, df_y_ref, df_y_test = sklearn.model_selection.train_test_split(\n",
    "        df_x, df_y, test_size=0.7, shuffle=False)\n",
    "    \n",
    "    df_x_ref_num, df_x_ref_cat = accepting.divide_numeric_categorical(df_x_ref)\n",
    "    df_x_test_num, df_x_test_cat = accepting.divide_numeric_categorical(df_x_test)\n",
    "    \n",
    "    ref_index = df_x_ref_cat.index\n",
    "    test_index = df_x_test_cat.index\n",
    "    encoder = TargetEncoder()\n",
    "    encoder.fit(df_x_ref_cat, df_y_ref)\n",
    "    df_x_ref_cat_transformed = pd.DataFrame(encoder.transform(df_x_ref_cat))\n",
    "    df_x_test_cat_transformed = pd.DataFrame(encoder.transform(df_x_test_cat))\n",
    "    df_x_ref_cat_transformed.set_index(ref_index, inplace=True)\n",
    "    df_x_test_cat_transformed.set_index(test_index, inplace=True)\n",
    "    \n",
    "    reference_data = df_x_ref_num.join(df_x_ref_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    testing_data = df_x_test_num.join(df_x_test_cat_transformed, lsuffix='_num').to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(reference_data)\n",
    "    reference_data = scaler.transform(reference_data)\n",
    "    testing_data = scaler.transform(testing_data)\n",
    "    \n",
    "    num_ref_batches = 3\n",
    "    num_test_batches = 7\n",
    "    ref_batches = np.array_split(reference_data, num_ref_batches)\n",
    "    test_batches = np.array_split(testing_data, num_test_batches)\n",
    "    \n",
    "    agraw1_target_reference_batches[file_path] = ref_batches\n",
    "    agraw1_target_testing_batches[file_path] = test_batches\n",
    "    \n",
    "print('agraw1 target')\n",
    "print(agraw1_target_reference_batches)\n",
    "print(agraw1_target_testing_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import kmeans_verbose_helpers\n",
    "\n",
    "\n",
    "def write_kmeans_results_ucdd_helper(output_filename_no_extension, ref_batches, n_init, max_iter, tol, random_state):\n",
    "    # dummy = [np.asarray(1), np.asarray(2), np.asarray(3)]\n",
    "    combinations = []\n",
    "    for i in range(3):\n",
    "    #     combinations.append(np.vstack((dummy[i], dummy[(i + 1) % 3])))\n",
    "        combinations.append(np.vstack((ref_batches[i], ref_batches[(i + 1) % 3])))\n",
    "        \n",
    "    all_results_from_combinations = []\n",
    "    for i, combination in enumerate(combinations):\n",
    "        filename = output_filename_no_extension + str(i) + '.txt'\n",
    "        print('filename', filename)\n",
    "        kmeans_verbose_helpers.write_verbose_kmeans_to_file(result_filename=output_filename_no_extension + str(i) + '.txt',\n",
    "                                     data_to_cluster=combination,\n",
    "                                     n_clusters=2, n_init=n_init, max_iter=max_iter, tol=tol, random_state=random_state)\n",
    "        output_dicts = kmeans_verbose_helpers.convert_kmeans_output_file_to_dicts(filename, n_init=n_init)\n",
    "        all_results_from_combinations.append(output_dicts)\n",
    "        kmeans_verbose_helpers.print_stats_from_kmeans_output_dicts(output_dicts)\n",
    "        \n",
    "    kmeans_verbose_helpers.print_stats_from_all_combinations(all_results_from_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 with categories excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best tol and max_iter (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename agraw1_exclude_new_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 18\n",
      "minimum initial inertia: 8347.093515830069\n",
      "maximum initial inertia: 18690.074352335017\n",
      "number of unique final inertia values: 3\n",
      "minimum final inertia: 6832.419247273033\n",
      "maximum final inertia: 6832.419270386381\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_exclude_new_output1.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 18\n",
      "minimum initial inertia: 8747.52244428163\n",
      "maximum initial inertia: 17134.178458342543\n",
      "number of unique final inertia values: 3\n",
      "minimum final inertia: 6820.074930844902\n",
      "maximum final inertia: 6820.0750120817775\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_exclude_new_output2.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 15\n",
      "minimum initial inertia: 8327.374918776613\n",
      "maximum initial inertia: 17457.421016637192\n",
      "number of unique final inertia values: 2\n",
      "minimum final inertia: 6806.4197402168365\n",
      "maximum final inertia: 6806.419740216837\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "{'total_max_iterations': 18, 'total_min_init_inertia': 8327.374918776613, 'total_max_init_inertia': 18690.074352335017, 'total_min_final_inertia': 6806.4197402168365, 'total_max_final_inertia': 6832.419270386381, 'total_num_convergences': 300, 'total_num_strict_convergences': 300, 'total_num_tol_based_convergences': 0}\n"
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('agraw1_exclude_new_output', agraw1_exclude_reference_batches[abrupt_agraw1_path],\n",
    "                                 n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch_strategy TrainBatchStrategies.SUBMAJORITY\n",
      "random_state\n",
      "0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "train_batch_strategy TrainBatchStrategies.SUBMAJORITY\n",
      "random_state\n",
      "100\n",
      "n_init 100 max_iter 18000 tol 0\n",
      "n_init 100 max_iter 18000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 31\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# for agraw1_path in all_agraw1_data_paths:\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m#         ucdd_eval.all_drifting_batches_randomness_robust(\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m#         'latency_std_err': latency_std_err\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m#     }\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agraw1_path \u001B[38;5;129;01min\u001B[39;00m all_agraw1_data_paths:\n\u001B[0;32m     30\u001B[0m     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m---> 31\u001B[0m         \u001B[43mucdd_eval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches_randomness_robust\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[43magraw1_exclude_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43magraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43magraw1_exclude_testing_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43magraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTrainBatchStrategies\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSUBMAJORITY\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m18000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_drift_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_runs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[0;32m     41\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     agraw1_exclude_stats[agraw1_path] \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mruns_results_bool\u001B[39m\u001B[38;5;124m'\u001B[39m: runs_results_bool,\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinal_fpr_mean\u001B[39m\u001B[38;5;124m'\u001B[39m: final_fpr_mean,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     47\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatency_std_err\u001B[39m\u001B[38;5;124m'\u001B[39m: latency_std_err\n\u001B[0;32m     48\u001B[0m     }\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAGRAW1 STATS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd_eval.py:57\u001B[0m, in \u001B[0;36mall_drifting_batches_randomness_robust\u001B[1;34m(reference_data_batches, testing_data_batches, train_batch_strategy, additional_check, n_init, max_iter, tol, true_drift_idx, first_random_state, min_runs, std_err_threshold)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m num_runs \u001B[38;5;241m<\u001B[39m min_runs \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(fpr_std_err, latency_std_err) \u001B[38;5;241m>\u001B[39m std_err_threshold:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_batch_strategy\u001B[39m\u001B[38;5;124m'\u001B[39m, train_batch_strategy)\n\u001B[1;32m---> 57\u001B[0m     drifting_batches_bool \u001B[38;5;241m=\u001B[39m \u001B[43mucdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreference_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtesting_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# print('drifting_batches_bool')\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# print(drifting_batches_bool)\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     drift_locations \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(drifting_batches_bool))[drifting_batches_bool]\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:189\u001B[0m, in \u001B[0;36mall_drifting_batches\u001B[1;34m(reference_data_batches, testing_data_batches, train_batch_strategy, additional_check, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m    187\u001B[0m num_ref_drifts \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# how many training batches signal drift against this testing batch\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, ref_window \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(reference_data_batches):\n\u001B[1;32m--> 189\u001B[0m     drift_here \u001B[38;5;241m=\u001B[39m \u001B[43mconcept_drift_detected\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drift_here:\n\u001B[0;32m    192\u001B[0m         num_ref_drifts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:142\u001B[0m, in \u001B[0;36mconcept_drift_detected\u001B[1;34m(ref_window, test_window, additional_check, n_init, max_iter, tol, random_state, threshold, debug)\u001B[0m\n\u001B[0;32m    137\u001B[0m ref_plus, ref_minus, test_plus, test_minus \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m    138\u001B[0m     join_predict_split(ref_window, test_window,\n\u001B[0;32m    139\u001B[0m                        n_init\u001B[38;5;241m=\u001B[39mn_init, max_iter\u001B[38;5;241m=\u001B[39mmax_iter, tol\u001B[38;5;241m=\u001B[39mtol, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBETA MINUS (ref+, ref-, test-)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 142\u001B[0m beta_minus, beta_minus_additional \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_beta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mref_plus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mref_minus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_minus\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBETA PLUS (ref-, ref+, test+)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    145\u001B[0m beta_plus, beta_plus_additional \u001B[38;5;241m=\u001B[39m compute_beta(\n\u001B[0;32m    146\u001B[0m     ref_minus, ref_plus, test_plus)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:103\u001B[0m, in \u001B[0;36mcompute_beta\u001B[1;34m(u, v0, v1, beta_x, debug)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03mFind neighbors and compute beta based on u, v0, v1\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;124;03m    beta_additional - the beta cdf value for exchanged numbers of neighbours as parameters)\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    102\u001B[0m w0 \u001B[38;5;241m=\u001B[39m compute_neighbors(u, v0, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 103\u001B[0m w1 \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_neighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mv1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneighbors in W0\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mlen\u001B[39m(w0))\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneighbors in W1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mlen\u001B[39m(w1))\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:82\u001B[0m, in \u001B[0;36mcompute_neighbors\u001B[1;34m(u, v, debug_string)\u001B[0m\n\u001B[0;32m     79\u001B[0m neigh \u001B[38;5;241m=\u001B[39m NearestNeighbors(n_neighbors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     80\u001B[0m neigh\u001B[38;5;241m.\u001B[39mfit(v)\n\u001B[1;32m---> 82\u001B[0m neigh_ind_v \u001B[38;5;241m=\u001B[39m \u001B[43mneigh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkneighbors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;66;03m# print('neigh_ind_' + debug_string, neigh_ind_v)\u001B[39;00m\n\u001B[0;32m     84\u001B[0m unique_v_neighbor_indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(neigh_ind_v)\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py:814\u001B[0m, in \u001B[0;36mKNeighborsMixin.kneighbors\u001B[1;34m(self, X, n_neighbors, return_distance)\u001B[0m\n\u001B[0;32m    808\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m issparse(X):\n\u001B[0;32m    809\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    810\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m does not work with sparse matrices. Densify the data, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    811\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor set algorithm=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrute\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    812\u001B[0m             \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_method\n\u001B[0;32m    813\u001B[0m         )\n\u001B[1;32m--> 814\u001B[0m     chunked_results \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_tree_query_parallel_helper\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_distance\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mgen_even_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    820\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    821\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minternal: _fit_method not recognized\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\joblib\\parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1098\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\joblib\\parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(\u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    766\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n\u001B[0;32m    767\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py:762\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    761\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 762\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:574\u001B[0m, in \u001B[0;36mEvent.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    572\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[1;32m--> 574\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "from core import ucdd_supported_parameters as spms\n",
    "\n",
    "\n",
    "\n",
    "agraw1_exclude_stats = {}\n",
    "agraw1_path = all_agraw1_data_paths[-1]\n",
    "# for agraw1_path in all_agraw1_data_paths:\n",
    "#     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "#         ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "#         agraw1_reference_batches[agraw1_path],\n",
    "#         agraw1_testing_batches[agraw1_path],\n",
    "#         train_batch_strategy=spms.TrainBatchStrategies.ANY,\n",
    "#         additional_check=True,\n",
    "#         n_init=100,\n",
    "#         max_iter=18000,\n",
    "#         tol=0,\n",
    "#         true_drift_idx=2,\n",
    "#         min_runs=2\n",
    "#     )\n",
    "#     agraw1_stats[agraw1_path] = {\n",
    "#         'runs_results_bool': runs_results_bool,\n",
    "#         'final_fpr_mean': final_fpr_mean,\n",
    "#         'fpr_std_err': fpr_std_err,\n",
    "#         'final_latency_mean': final_latency_mean,\n",
    "#         'latency_std_err': latency_std_err\n",
    "#     }\n",
    "    \n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_exclude_reference_batches[agraw1_path],\n",
    "        agraw1_exclude_testing_batches[agraw1_path],\n",
    "        train_batch_strategy=spms.TrainBatchStrategies.SUBMAJORITY,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=18000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    agraw1_exclude_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 STATS')\n",
    "print(agraw1_exclude_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the obtained results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'train_batch_strategy': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in agraw1_exclude_stats.items():\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'exclude'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['train_batch_strategy'].append('submajority')\n",
    "    final_result_dict['additional_check'].append('yes')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(18000)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('agraw1_exclude_jupyter_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 with categories onehot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best tol and max_iter (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename agraw1_onehot_new_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 34\n",
      "minimum initial inertia: 104954.65523014727\n",
      "maximum initial inertia: 116645.01047187511\n",
      "number of unique final inertia values: 19\n",
      "minimum final inertia: 58341.71044879218\n",
      "maximum final inertia: 59761.482370558224\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_onehot_new_output1.txt\n",
      "random state: 1053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of results: 100\n",
      "maximum number of iterations: 33\n",
      "minimum initial inertia: 105187.473271114\n",
      "maximum initial inertia: 117235.7046827969\n",
      "number of unique final inertia values: 13\n",
      "minimum final inertia: 58307.80854907961\n",
      "maximum final inertia: 59757.571876885886\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_onehot_new_output2.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 26\n",
      "minimum initial inertia: 104976.092943036\n",
      "maximum initial inertia: 118204.3234742695\n",
      "number of unique final inertia values: 23\n",
      "minimum final inertia: 58330.68910892084\n",
      "maximum final inertia: 59758.880442981055\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "{'total_max_iterations': 34, 'total_min_init_inertia': 104954.65523014727, 'total_max_init_inertia': 118204.3234742695, 'total_min_final_inertia': 58307.80854907961, 'total_max_final_inertia': 59761.482370558224, 'total_num_convergences': 300, 'total_num_strict_convergences': 300, 'total_num_tol_based_convergences': 0}\n"
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('agraw1_onehot_new_output', agraw1_onehot_reference_batches[abrupt_agraw1_path],\n",
    "                                 n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "train_batch_strategy TrainBatchStrategies.MAJORITY\n",
      "random_state\n",
      "0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainBatchStrategies.MAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "TrainBatchStrategies.MAJORITY\n",
      "train_batch_strategy None\n",
      "acceptable_strategies:\n",
      "TrainBatchStrategies.ANY\n",
      "TrainBatchStrategies.SUBMAJORITY\n",
      "TrainBatchStrategies.MAJORITY\n",
      "TrainBatchStrategies.ALL\n",
      "n_init 100 max_iter 34000 tol 0\n",
      "n_init 100 max_iter 34000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_init 100 max_iter 34000 tol 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 10\u001B[0m\n\u001B[0;32m      6\u001B[0m agraw1_onehot_stats \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agraw1_path \u001B[38;5;129;01min\u001B[39;00m all_agraw1_data_paths:\n\u001B[0;32m      9\u001B[0m     runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m---> 10\u001B[0m         \u001B[43mucdd_eval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches_randomness_robust\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43magraw1_onehot_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43magraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43magraw1_onehot_testing_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43magraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTrainBatchStrategies\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMAJORITY\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m34000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrue_drift_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_runs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[0;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     agraw1_onehot_stats[agraw1_path] \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m     22\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mruns_results_bool\u001B[39m\u001B[38;5;124m'\u001B[39m: runs_results_bool,\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinal_fpr_mean\u001B[39m\u001B[38;5;124m'\u001B[39m: final_fpr_mean,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatency_std_err\u001B[39m\u001B[38;5;124m'\u001B[39m: latency_std_err\n\u001B[0;32m     27\u001B[0m     }\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAGRAW1 STATS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd_eval.py:57\u001B[0m, in \u001B[0;36mall_drifting_batches_randomness_robust\u001B[1;34m(reference_data_batches, testing_data_batches, train_batch_strategy, additional_check, n_init, max_iter, tol, true_drift_idx, first_random_state, min_runs, std_err_threshold)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m num_runs \u001B[38;5;241m<\u001B[39m min_runs \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(fpr_std_err, latency_std_err) \u001B[38;5;241m>\u001B[39m std_err_threshold:\n\u001B[0;32m     56\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_batch_strategy\u001B[39m\u001B[38;5;124m'\u001B[39m, train_batch_strategy)\n\u001B[1;32m---> 57\u001B[0m     drifting_batches_bool \u001B[38;5;241m=\u001B[39m \u001B[43mucdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_drifting_batches\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreference_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtesting_data_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_batch_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;66;03m# print('drifting_batches_bool')\u001B[39;00m\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# print(drifting_batches_bool)\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     drift_locations \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;28mlen\u001B[39m(drifting_batches_bool))[drifting_batches_bool]\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:189\u001B[0m, in \u001B[0;36mall_drifting_batches\u001B[1;34m(reference_data_batches, testing_data_batches, train_batch_strategy, additional_check, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m    187\u001B[0m num_ref_drifts \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# how many training batches signal drift against this testing batch\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, ref_window \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(reference_data_batches):\n\u001B[1;32m--> 189\u001B[0m     drift_here \u001B[38;5;241m=\u001B[39m \u001B[43mconcept_drift_detected\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_check\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drift_here:\n\u001B[0;32m    192\u001B[0m         num_ref_drifts \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:138\u001B[0m, in \u001B[0;36mconcept_drift_detected\u001B[1;34m(ref_window, test_window, additional_check, n_init, max_iter, tol, random_state, threshold, debug)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcept_drift_detected\u001B[39m(\n\u001B[0;32m    113\u001B[0m         ref_window,\n\u001B[0;32m    114\u001B[0m         test_window,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    121\u001B[0m         debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    122\u001B[0m ):\n\u001B[0;32m    123\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;124;03m    Detect whether a concept drift occurred based on one reference and one testing window\u001B[39;00m\n\u001B[0;32m    125\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m    :return: true if drift is detected based on the two windows, false otherwise\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    137\u001B[0m     ref_plus, ref_minus, test_plus, test_minus \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m--> 138\u001B[0m         \u001B[43mjoin_predict_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mref_window\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_window\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m debug: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBETA MINUS (ref+, ref-, test-)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    142\u001B[0m     beta_minus, beta_minus_additional \u001B[38;5;241m=\u001B[39m compute_beta(\n\u001B[0;32m    143\u001B[0m         ref_plus, ref_minus, test_minus)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\core\\ucdd.py:63\u001B[0m, in \u001B[0;36mjoin_predict_split\u001B[1;34m(ref_window, test_window, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_init\u001B[39m\u001B[38;5;124m'\u001B[39m, n_init, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_iter\u001B[39m\u001B[38;5;124m'\u001B[39m, max_iter, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtol\u001B[39m\u001B[38;5;124m'\u001B[39m, tol)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# predict their label values\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m \u001B[43mKMeans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwindow_union\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# split values by predicted label and window\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m split_back_to_windows(window_union, predicted_labels, ref_window\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], test_window\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:996\u001B[0m, in \u001B[0;36m_BaseKMeans.fit_predict\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    974\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001B[39;00m\n\u001B[0;32m    975\u001B[0m \n\u001B[0;32m    976\u001B[0m \u001B[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    994\u001B[0m \u001B[38;5;124;03m        Index of the cluster each sample belongs to.\u001B[39;00m\n\u001B[0;32m    995\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 996\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1410\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1406\u001B[0m best_inertia, best_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1408\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_init):\n\u001B[0;32m   1409\u001B[0m     \u001B[38;5;66;03m# Initialize centers\u001B[39;00m\n\u001B[1;32m-> 1410\u001B[0m     centers_init \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_init_centroids\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m   1412\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1413\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[0;32m   1414\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitialization complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:952\u001B[0m, in \u001B[0;36m_BaseKMeans._init_centroids\u001B[1;34m(self, X, x_squared_norms, init, random_state, init_size, n_centroids)\u001B[0m\n\u001B[0;32m    949\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(init, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m init \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mk-means++\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 952\u001B[0m     centers, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_kmeans_plusplus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    954\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(init, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m init \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrandom\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    959\u001B[0m     seeds \u001B[38;5;241m=\u001B[39m random_state\u001B[38;5;241m.\u001B[39mpermutation(n_samples)[:n_clusters]\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:224\u001B[0m, in \u001B[0;36m_kmeans_plusplus\u001B[1;34m(X, n_clusters, x_squared_norms, random_state, n_local_trials)\u001B[0m\n\u001B[0;32m    221\u001B[0m np\u001B[38;5;241m.\u001B[39mclip(candidate_ids, \u001B[38;5;28;01mNone\u001B[39;00m, closest_dist_sq\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, out\u001B[38;5;241m=\u001B[39mcandidate_ids)\n\u001B[0;32m    223\u001B[0m \u001B[38;5;66;03m# Compute distances to center candidates\u001B[39;00m\n\u001B[1;32m--> 224\u001B[0m distance_to_candidates \u001B[38;5;241m=\u001B[39m \u001B[43m_euclidean_distances\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcandidate_ids\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_norm_squared\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msquared\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m    226\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;66;03m# update closest distances squared and potential for each candidate\u001B[39;00m\n\u001B[0;32m    229\u001B[0m np\u001B[38;5;241m.\u001B[39mminimum(closest_dist_sq, distance_to_candidates, out\u001B[38;5;241m=\u001B[39mdistance_to_candidates)\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:369\u001B[0m, in \u001B[0;36m_euclidean_distances\u001B[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001B[0m\n\u001B[0;32m    366\u001B[0m     distances \u001B[38;5;241m=\u001B[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001B[0;32m    367\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    368\u001B[0m     \u001B[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001B[39;00m\n\u001B[1;32m--> 369\u001B[0m     distances \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43msafe_sparse_dot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdense_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    370\u001B[0m     distances \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m XX\n\u001B[0;32m    371\u001B[0m     distances \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m YY\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:152\u001B[0m, in \u001B[0;36msafe_sparse_dot\u001B[1;34m(a, b, dense_output)\u001B[0m\n\u001B[0;32m    150\u001B[0m         ret \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(a, b)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 152\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    155\u001B[0m     sparse\u001B[38;5;241m.\u001B[39missparse(a)\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(b)\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m dense_output\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(ret, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoarray\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    159\u001B[0m ):\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39mtoarray()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "from core import ucdd_supported_parameters as spms\n",
    "\n",
    "\n",
    "print('...')\n",
    "agraw1_onehot_stats = {}\n",
    "\n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_onehot_reference_batches[agraw1_path],\n",
    "        agraw1_onehot_testing_batches[agraw1_path],\n",
    "        train_batch_strategy=spms.TrainBatchStrategies.MAJORITY,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=34000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    agraw1_onehot_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 STATS')\n",
    "print(agraw1_onehot_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the obtained results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'train_batch_strategy': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in agraw1_onehot_stats.items():\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'onehot'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['train_batch_strategy'].append('majority')\n",
    "    final_result_dict['additional_check'].append('yes')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(34000)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('agraw1_onehot_jupyter_results_majority.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGRAW1 with categories target encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best tol and max_iter (the drift type is irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename agraw1_target_new_output0.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 18\n",
      "minimum initial inertia: 17607.98488650251\n",
      "maximum initial inertia: 29056.647428908935\n",
      "number of unique final inertia values: 21\n",
      "minimum final inertia: 13299.613663873319\n",
      "maximum final inertia: 14276.6351586479\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_target_new_output1.txt\n",
      "random state: 1053\n",
      "total number of results: 100\n",
      "maximum number of iterations: 22\n",
      "minimum initial inertia: 17728.884037917425\n",
      "maximum initial inertia: 30906.112923249697\n",
      "number of unique final inertia values: 13\n",
      "minimum final inertia: 13290.390633663217\n",
      "maximum final inertia: 14285.170980526265\n",
      "total number of convergences: 100\n",
      "number of strict convergences: 100\n",
      "number of tol-based convergences: 0\n",
      "filename agraw1_target_new_output2.txt\n",
      "random state: 1053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mwrite_kmeans_results_ucdd_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43magraw1_target_new_output\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magraw1_target_reference_batches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mabrupt_agraw1_path\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1053\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 15\u001B[0m, in \u001B[0;36mwrite_kmeans_results_ucdd_helper\u001B[1;34m(output_filename_no_extension, ref_batches, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     13\u001B[0m filename \u001B[38;5;241m=\u001B[39m output_filename_no_extension \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m, filename)\n\u001B[1;32m---> 15\u001B[0m \u001B[43mkmeans_verbose_helpers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_verbose_kmeans_to_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_filename_no_extension\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mdata_to_cluster\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcombination\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m output_dicts \u001B[38;5;241m=\u001B[39m kmeans_verbose_helpers\u001B[38;5;241m.\u001B[39mconvert_kmeans_output_file_to_dicts(filename, n_init\u001B[38;5;241m=\u001B[39mn_init)\n\u001B[0;32m     19\u001B[0m all_results_from_combinations\u001B[38;5;241m.\u001B[39mappend(output_dicts)\n",
      "File \u001B[1;32m~\\PycharmProjects\\clustering-drift-detection\\ucdd_improved\\eval_helpers\\kmeans_verbose_helpers.py:32\u001B[0m, in \u001B[0;36mwrite_verbose_kmeans_to_file\u001B[1;34m(result_filename, data_to_cluster, n_clusters, n_init, max_iter, tol, random_state)\u001B[0m\n\u001B[0;32m     29\u001B[0m orig_stdout \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mstdout\n\u001B[0;32m     30\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(result_filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 32\u001B[0m fitted_kmeans \u001B[38;5;241m=\u001B[39m \u001B[43mKMeans\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_clusters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_to_cluster\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m sys\u001B[38;5;241m.\u001B[39mstdout \u001B[38;5;241m=\u001B[39m orig_stdout\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001B[0m, in \u001B[0;36mKMeans.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m   1414\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInitialization complete\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1416\u001B[0m \u001B[38;5;66;03m# run a k-means once\u001B[39;00m\n\u001B[1;32m-> 1417\u001B[0m labels, inertia, centers, n_iter_ \u001B[38;5;241m=\u001B[39m \u001B[43mkmeans_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1418\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1419\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1420\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcenters_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1421\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1422\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1423\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1424\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_squared_norms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_squared_norms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1425\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_n_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1426\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1428\u001B[0m \u001B[38;5;66;03m# determine if these results are the best so far\u001B[39;00m\n\u001B[0;32m   1429\u001B[0m \u001B[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001B[39;00m\n\u001B[0;32m   1430\u001B[0m \u001B[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001B[39;00m\n\u001B[0;32m   1431\u001B[0m \u001B[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001B[39;00m\n\u001B[0;32m   1432\u001B[0m \u001B[38;5;66;03m# permuted labels, due to rounding errors)\u001B[39;00m\n\u001B[0;32m   1433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_inertia \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   1434\u001B[0m     inertia \u001B[38;5;241m<\u001B[39m best_inertia\n\u001B[0;32m   1435\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_same_clustering(labels, best_labels, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters)\n\u001B[0;32m   1436\u001B[0m ):\n",
      "File \u001B[1;32mc:\\users\\jpohl\\pycharmprojects\\clustering-drift-detection\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:661\u001B[0m, in \u001B[0;36m_kmeans_single_lloyd\u001B[1;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001B[0m\n\u001B[0;32m    648\u001B[0m lloyd_iter(\n\u001B[0;32m    649\u001B[0m     X,\n\u001B[0;32m    650\u001B[0m     sample_weight,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    657\u001B[0m     n_threads,\n\u001B[0;32m    658\u001B[0m )\n\u001B[0;32m    660\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[1;32m--> 661\u001B[0m     inertia \u001B[38;5;241m=\u001B[39m \u001B[43m_inertia\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcenters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    662\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, inertia \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minertia\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    664\u001B[0m centers, centers_new \u001B[38;5;241m=\u001B[39m centers_new, centers\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "write_kmeans_results_ucdd_helper('agraw1_target_new_output', agraw1_target_reference_batches[abrupt_agraw1_path],\n",
    "                                 n_init=100, max_iter=500, tol=0,\n",
    "                                 random_state=1053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use them for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<__array_function__ internals>:177\u001B[0m, in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from core import ucdd_eval\n",
    "from core import ucdd_supported_parameters as spms\n",
    "\n",
    "\n",
    "\n",
    "agraw1_target_stats = {}\n",
    "    \n",
    "for agraw1_path in all_agraw1_data_paths:\n",
    "    runs_results_bool, final_fpr_mean, fpr_std_err, final_latency_mean, latency_std_err = \\\n",
    "        ucdd_eval.all_drifting_batches_randomness_robust(\n",
    "        agraw1_target_reference_batches[agraw1_path],\n",
    "        agraw1_target_testing_batches[agraw1_path],\n",
    "        train_batch_strategy=spms.TrainBatchStrategies.MAJORITY,\n",
    "        additional_check=True,\n",
    "        n_init=100,\n",
    "        max_iter=45000,\n",
    "        tol=0,\n",
    "        true_drift_idx=2,\n",
    "        min_runs=2\n",
    "    )\n",
    "    agraw1_target_stats[agraw1_path] = {\n",
    "        'runs_results_bool': runs_results_bool,\n",
    "        'final_fpr_mean': final_fpr_mean,\n",
    "        'fpr_std_err': fpr_std_err,\n",
    "        'final_latency_mean': final_latency_mean,\n",
    "        'latency_std_err': latency_std_err\n",
    "    }\n",
    "\n",
    "print('AGRAW1 STATS')\n",
    "print(agraw1_target_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the obtained results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_helpers import helpers\n",
    "\n",
    "\n",
    "final_result_dict = {\n",
    "    'type_of_data': [], 'dataset': [], 'drift': [], 'width': [], 'encoding': [],\n",
    "    'train_batch_strategy': [], 'additional_check': [],\n",
    "    'n_init': [], 'max_iter': [], 'tol': [],\n",
    "    'FPR_mean': [], 'latency_mean': []\n",
    "}\n",
    "\n",
    "for data_path, stats_dict in agraw1_target_stats.items():\n",
    "    synthetic_filename_info = helpers.synthetic_data_information(data_path)\n",
    "    encoding = 'target'\n",
    "    fpr_mean = float(stats_dict['final_fpr_mean'])\n",
    "    latency_mean = float(stats_dict['final_latency_mean'])\n",
    "    \n",
    "    final_result_dict['type_of_data'].append(synthetic_filename_info['type_of_data'])\n",
    "    final_result_dict['dataset'].append(synthetic_filename_info['dataset_name'])\n",
    "    final_result_dict['drift'].append(synthetic_filename_info['drift_type'])\n",
    "    final_result_dict['width'].append(synthetic_filename_info['drift_width'])\n",
    "    final_result_dict['encoding'].append(encoding)\n",
    "    final_result_dict['train_batch_strategy'].append('majority')\n",
    "    final_result_dict['additional_check'].append('yes')\n",
    "    final_result_dict['n_init'].append(100)\n",
    "    final_result_dict['max_iter'].append(45000)\n",
    "    final_result_dict['tol'].append(0)\n",
    "    final_result_dict['FPR_mean'].append(fpr_mean)\n",
    "    final_result_dict['latency_mean'].append(latency_mean)\n",
    "    \n",
    "final_result_df = pd.DataFrame.from_dict(final_result_dict)\n",
    "sorted_final_result_df = final_result_df.sort_values(['drift', 'dataset', 'encoding', 'width'])\n",
    "final_result_df.to_csv('agraw1_target_jupyter_results_majority.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}